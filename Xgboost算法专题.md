# Xgboost算法原理
- XGBoost是一个具有可扩展性的树提升算法机器学习系统，它的可扩展性体现在以下四个方面（该算法开发者陈天奇《XGBoost: A Scalable Tree Boosting System》）：
	- 模型的scalability，弱分类器除cart外也支持lr和linear。
	策略的scalability，可以支持不同的loss functions，来优化效果，只要一、二阶可导即可。
	- 算法的scalability，做了很多细节工作，来优化参数学习和迭代速度，特征压缩技，bagging学习中的特征抽样，特征选择与阈值分裂的分位方法和并行方法等。
	- 数据的scalability，因为3中的优化，支持B级别的快速训练和建模；同时也因为加上了正则项和随机特征抽样，减少了过拟合问题。
	- 
XGBoost的主要优势及创新根据作者的描述，可以归纳为以下几点：

	- 一个新奇的（novel）处理稀疏数据的树学习算法。
	- 使用近似树学习（ approximate tree learning）理论利用分位数的描述，可以给每一个训练实例一个合理的加权权重。
	- 并行和分布式的设计使得学习速度非常快，建模更快速。
	- XGBoost exploits out-of-core computation 机制让仅使用笔记本就可以处理数以亿计的数据成为可能。
	- 端到端系统使用最小的集群资源就可以处理大量的数据。


- XGBoost被用于监督学习，监督学习指利用具有多个特征的训练数据xi来预测目标变量yi，在训练数据中yi作为标签是给定的。





## 一、损失函数
- 我们比较常见的模型如线性模型（包括线性回归和logistic regression）采用了线性叠加的方式进行预测。
对于最简单的线性模型，它的模型结构如下：

![](https://i.imgur.com/xlCIfRm.png)

- 它是一条组合输入特征权重的直线。预测值根据任务的不同具有不同的解释，如回归和分类。
- 在线性回归问题中，参数就是每一个特征的系数theta 。通常，我们使用theta 来表示参数。

- 目标函数：训练损失+正则化
	- 定义一个目标函数来衡量模型学习到的参数的好坏。
	- 损失函数：测量模型在训练数据上如何学习
	- 正则项：用于控制模型的复杂度，也就是防止模型过拟合。

- 模型和参数本身指定了给定输入我们如何做预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数登场了。一般的目标函数包含下面两项：

![](https://i.imgur.com/kjqTIrC.png)

![](https://i.imgur.com/s5JzLzM.png)

![](https://i.imgur.com/xrStNn8.png)

![](https://i.imgur.com/8Eqo2Jb.png)

## 二、分裂结点算法

- exact greedy algorithm—贪心算法获取最优切分点
- approximate algorithm— 近似算法，提出了候选分割点概念，先通过直方图算法获得候选分割点的分布情况，然后根据候选分割点将连续的特征信息映射到不同的buckets中，并统计汇总信息
- Weighted Quantile Sketch—分布式加权直方图算法


## 三、正则化

- 损失函数中加入了正则项，可见公式
- 样本采样和列采样

## 四、对缺失值处理

通常情况下，我们人为在处理缺失值的时候大多会选用中位数、均值或是二者的融合来对数值型特征进行填补，使用出现次数最多的类别来填补缺失的类别特征。

很多的机器学习算法都无法提供缺失值的自动处理，都需要人为地去处理，但是xgboost模型却能够处理缺失值，也就是说模型允许缺失值存在。

 

原是论文中关于缺失值的处理将其看与稀疏矩阵的处理看作一样。在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率。如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树。

## 五、优缺点

1、正则化

标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。

实际上，XGBoost以“正则化提升(regularized boosting)”技术而闻名。

2、并行处理

XGBoost可以实现并行处理，相比GBM有了速度的飞跃，LightGBM也是微软最新推出的一个速度提升的算法。 XGBoost也支持Hadoop实现。

3、高度的灵活性

XGBoost 允许用户定义自定义优化目标和评价标准 。

4、缺失值处理

XGBoost内置处理缺失值的规则。用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。

5、剪枝

当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。

这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。

6、内置交叉验证

XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。

而GBM使用网格搜索，只能检测有限个值。

7、在已有的模型基础上继续

XGBoost可以在上一轮的结果上继续训练。

sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。
## 六、应用场景

- Xgboost是boost集成算法之一，在gbdt算法基础上增加并行化处理，分类回归效果都挺不错，在各大比赛上所使用的集成算法大部分是Xgboost，有很好的实用性。

## 七、sklearn参数

- `eta`[deault=0.3,又称：学习率]
    + 更新中使用的步长缩小以防止过度拟合。 在每个增强步骤之后，我们可以直接获得新特征的权重，并且eta缩小特征权重以使增强过程更加保守。
    + range[0,1]，范围一般在(0,1)之间

- `gamma`[default=0,又称：最小分割损失]
    + 在树的叶节点上进行进一步分区所需的最小损耗减少。`gamma`越大，算法就越保守。
    + range[0,∞]，范围在(0,∞)

- `max_depth`[default=6]
    + 树的最大深度。 增加此值将使模型更复杂，更容易过度拟合。 当tree_method设置为hist并且它表示没有深度限制时，0仅在损失的增长策略中被接受。 请注意，在训练深树时，XGBoost会极大地消耗内存。
    + range[0,∞],(当tree_method设置为hist时，0仅在损失的增长策略中被接受)

- `min_child_weight`[default=1]
    + z子树所需的实例重量（hessian）的最小总和。 如果树分裂步骤导致叶节点的实例权重之和小于min_child_weight，则构建过程将放弃进一步的分裂。 在线性回归任务中，这仅对应于每个节点中需要的最小实例数。 min_child_weight越大，算法越保守。
    + range[0,∞]，范围在(0,∞)

- `max_delta_step` [default=0]
    + 我们允许每个叶子输出的最大增量步长。 如果该值设置为0，则表示没有约束。 如果将其设置为正值，则可以帮助使更新步骤更加保守。 通常不需要此参数，但当类非常不平衡时，它可能有助于逻辑回归。 将其设置为值1-10可能有助于控制更新。
    + range: [0,∞],范围在(0,∞)

- subsample [default=1]
    + 训练实例的子样本比率。 将其设置为0.5意味着XGBoost会在生长树之前随机抽取一半训练数据。 这样可以防止过度拟合。 子采样将在每次提升迭代中发生一次。
    + range: （0,1],范围在(0,1)


- `colsample_bytree`, `colsample_bylevel`, `colsample_bynode` [default=1]
    + 这是用于列的子采样的一系列参数。
    + 所有`colsample_by *`参数的范围为（0,1），默认值为1，并指定要进行二次采样的列的分数。
    + `colsample_bytree`是构造每个树时列的子采样率。 对于构造的每个树，子采样发生一次。
    + `colsample_bylevel`是每个级别的列的子采样率。 对于树中达到的每个新深度级别，子采样都会发生一次。 列是从为当前树选择的列集中进行子采样的。
    + `colsample_bynode`是每个节点（拆分）的列的子采样率。 每次评估新的拆分时，都会发生一次子采样。 列是从为当前级别选择的列集中进行二次采样的。
    + `colsample_by *`参数累积工作。 例如，具有64个要素的组合{`colsample_bytree`：0.5，`colsample_bylevel`：0.5，`colsample_bynode`：0.5}将在每次拆分时留下8个要素供您选择。


- `lambda` [default=1, alias: reg_lambda]
    + 关于权重的L2正则化项。 增加此值将使模型更加保守。(更加平滑）

- `alpha` [default=0, alias: reg_alpha]
    + 关于权重的L1正则化项。 增加此值将使模型更加保守。(更加平滑)

- `tree_method` string [default= auto]
    + XGBoost中使用的树构造算法。 参见参考文献中的描述。
    + 参考链接：https://arxiv.org/abs/1603.02754
    + XGBoost支持hist和大规模分布式训练，仅支持大规模外部内存版本。
    + 选择: `auto`, `exact`, `approx`, `hist`, `gpu_hist`
        * `auto`： 对于中小型数据集，将使用精确的贪婪（精确）。
            * 对于非常大的数据集，将选择近似算法（近似）。
            * 因为旧版本总是在单个机器中使用精确贪婪，所以当选择近似算法来通知该选择时，用户将得到消息。
        * `exact`: 精确的贪婪算法。
        * `approx` ： 使用分位数草图和梯度直方图的近似贪婪算法。
        * `hist`：快速直方图优化近似贪心算法。 它使用了一些性能改进，例如垃圾箱缓存。
        * `gpu_hist`：hist算法的GPU实现。


## 参考链接

1、https://blog.csdn.net/zaishijizhidian/article/details/88203482

2、https://zhuanlan.zhihu.com/p/58221959

3、https://xgboost.readthedocs.io/en/latest/parameter.html

