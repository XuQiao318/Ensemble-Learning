# 集成学习算法专题
![](https://i.imgur.com/L5cdT40.png)

## 一、集成学习简介
### （一）集成学习的概念
- 集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统（multi-classifier system）、基于委员会的学习（committee-base learning）。
- 集成学习的一般结构：先产生一组“个体学习器”（individual learner），再用某种策略将它们结合起来。

- 个体学习器
	+ 个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等，此时集成中只包含同种类型的个体学习器，例如“决策树集成”中全是决策树，“神经网络集成”中全是神经网络，这样的集成是同质的。同质集成中的个体学习器亦称为“基学习器”（base learner），相应的学习算法亦称为“基学习算法”（base learning algorithm）。
	+ 集成也可包含不同类型的个体学习器，例如同时包含决策树和神经网络，这样的集成是异质的。异质集成中的个体学习器由不同的学习算法生成，这时不再有基学习算法；相应的，个体学习器一般不称为基学习器，常称为“组件学习器”或直接称为个体学习器。
	+ “好而不同”，要获得好的集成，个体学习器应“好而不同”。

![](https://i.imgur.com/U3O0BRc.png)



- 根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，分别为Boosting和Bagging。
	+ Boosting：即个体学习器之间存在强依赖关系，必须是串行生成的序列化方法；
	+ Bagging：个体学习器不存在强依赖关系，可同时生成的并行化方法。

### （二）Boosting【串行方法】

- Boosting是一族可将弱学习器提升为强学习器的算法。
- 过程：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学习器数目达到事先指定的值T， 最终将这T个基学习器进行加权结合。
- Boosting典型算法：Adaboost、GBDT、Xgboost
- Boosting 算法要求基学习器能对特定的数据分布进行学习，这可通过"重赋权法" (re-weighting)实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。
- 对无法接受带权样本的基学习算法，则可通过"重采样法" (re-sampling)来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练.
- 从偏差一方差分解的角度看， Boosting 主要关住降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成.

### （三）Bagging【并行方法】

- 给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器.这样，由于训练数据不同，我们获得的基学习器可望具有比较大的差异。
- 为获得好的集成，我们同时还希望个体学习器不能太差.如果来样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，甚至不足以进行有效学习，这显然无法确保产生出比较好的基学习器.为解决这个问题，我们可考虑使用相互有交叠的采样子集。
- Bagging是并行式集成学习方法最著名的代表.从名字即可看出，它直接基于自助来样法(bootstrap sampling).
	- Bootstrap：给定包含m 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m 个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现.初始训练集中约有63.2%的样本出现在来样集中.
- 自助采样过程还给Bagging 带来了另一个优点:由于每个基学习器只使用了初始训练集中约63.2% 的样本，剩下约36.8% 的样本可用作验证集来对泛化性能进行"包外估计" (out-oιbag estimate)，也就是测试集。
- 通过对m个样本使用Bootstrap，我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging 的基本流程.在对预测输出进行结合时， Bagging 通常对分类任务使用简单投票法，回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。
- Bagging既可以完成分类任务，也可以完成回归任务。
- 从偏差方差分解的角度看，Bagging 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。
	- 关于主要降低方差的解释：多个并行的模型，不同的样本特征结合起来可以很好地提高模型的泛化能力，有效的降低方差。

### （四）Boosting与Bagging的异同

- 算法过程上：
	- Boosting算法：对于训练集中的每个样本建立权值wi，表示对每个样本的权重， 其关键在与对于被错误分类的样本权重会在下一轮的分类中获得更大的权重（错误分类的样本的权重增加）。
		- 同时加大分类误差概率小的弱分类器的权值，使其在表决中起到更大的作用，减小分类误差率较大弱分类器的权值，使其在表决中起到较小的作用。每一次迭代都得到一个弱分类器，需要使用某种策略将其组合，最为最终模型，(adaboost给每个迭代之后的弱分类器一个权值，将其线性组合作为最终的分类器,误差小的分类器权值越大。)

	- Bagging算法：
		- 从原始样本集中使用Bootstraping 方法随机抽取n个训练样本，共进行k轮抽取，得到k个训练集（k个训练集之间相互独立，元素可以有重复）。
		- 对于n个训练集，我们训练k个模型，（这个模型可根据具体的情况而定，可以是决策树，knn等）。
		- 对于分类问题：由投票表决产生的分类结果；对于回归问题，由k个模型预测结果的均值作为最后预测的结果（所有模型的重要性相同）。

- 样本选择上：
	- Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是每个样本的权重。

- 样本权重上：
	- Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大。

- 预测函数上：
	- Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。

- 并行计算：
	- Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成.

### （五）  将决策树与以上框架组合成新的算法

- Bagging + 决策树 = 随机森林
- AdaBoost + 决策树 = 提升树
- gradient + 决策树 = GDBT


### （六） 结合策略

#### 6.1 平均法
- 对数值型输出hi (X) εIR， 最常见的结合策略是使用平均法(averaging).
	- 简单平均法(simple averaging)：简单的累加求均值的额方法
	- 加权平均法(weighted averaging)：为每个个体学习器分配权重累加，求和，求平均的方法
	- 显然，简单平均法是加权平均法wi = l/T 的特例.

#### 6.2 投票法
- 对分类任务来说，学习器hi 将从类别标记集合{Cl ，C2，... ， CN} 中预测出一个标记?最常见的结合策略是使用投票法(voting).
	- 绝对多数投票法：即若某标记得票过半数，则预测为该标记;否则拒绝预测.
	- 相对多数投票法(plurality voting)：即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。
	- 加权投票法：与加权平均法类似， Wi 是hi的权重，通常wi>0，累加和=1.
- 简单来说，投票法就是少数服从多数

#### 6.3 学习法

- 当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。stacking是学习法的典型代表。
- stacking本身是一种著名的集成学习方法，且有不少集成学习算法可视为器变体或特例。一般的资料介绍集成算法有三种，分别为boosting、bagging和stacking。西瓜书上将stacking算法放在此处进行介绍。

- Stacking 先从初始数据集训练出初级学习器，然后"生成"一个新数据集用于训练次级学习器.在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记.


## 二、随机森林（Random Forest）

### （一）Random Forest简介：随机选择样本，随机选择特征

- 随机森林(Random Forest ，简称RF) [Breiman, 2001a] 是Bagging的一个扩展变体.盯在以决策树为基学习器构建Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。【bootstrap+每个样本的随机属性选择】
- 传统决策树在选择划分属性时是在当前结点的属性集合(假定有d 个属性)中选择一个最优属性;而在RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分. 这里的参数k 控制了随机性的引入程度;若令k = d，则基决策树的构建与传统决策树相同;若令k = 1，则是随机选择一个属性用于划分; 一般情况下，推荐值k = log2d。
- 随机森林是bagging算法的进化版，改进的部分在于：
	- 随机森林使用CART决策树作为基学习器，为每个数据集建立完全分裂的决策树
	- 利用CART为每个数据集建立一个完全分裂、没有经过剪枝的决策树，最终得到多棵CART决策树。

### （二）Random Forest的优点

- 随机森林简单、容易实现、计算开销小，令人惊奇的是它在很多现实任务中展现出强大的性能，被誉为"代表集成学习技术水平的方法"可以看出，随机森林对Bagging 只做了小改动， 但是与Bagging中基学习器的"多样性"仅通过样本扰动(通过对初始训练集采样)而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。
- 随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。
- 随机森林的训练效率通常优于Bagging，因为在个体决策树的构建过程中， Bagging
使用的是" 确定型" 决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林使用的" 随机型"决策树则只需考察一个属性子集。

### （三）Random Forest的推广
由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。

#### 3.1 Extra trees

- extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：

	1） 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集。

	2） 在选定了划分特征后，RF的决策树会基于信息增益，基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会随机的选择一个特征值来划分决策树。

- 从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是bias相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好.

#### 3.2 Totally Random Trees Embedding
- Totally Random Trees Embedding(以下简称 TRTE)是一种非监督学习的数据转化方法。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。

- TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0, 0,0,1,0,0, 0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

- 映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。

#### 3.3 Isolation Forest
- Isolation Forest（以下简称IForest）是一种异常点检测的方法。它也使用了类似于RF的方法来检测异常点。

- 对于在T个决策树的样本集，IForest也会对训练集进行随机采样,但是采样个数不需要和RF一样，对于RF，需要采样到采样集样本个数等于训练集个数。但是IForest不需要采样这么多，一般来说，采样个数要远远小于训练集个数？为什么呢？因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。

- 对于每一个决策树的建立， IForest采用随机选择一个划分特征，对划分特征随机选择一个划分阈值。这点也和RF不同。

- 另外，IForest一般会选择一个比较小的最大决策树深度max_depth,原因同样本采集，用少量的异常点检测一般不需要这么大规模的决策树。

- 对于异常点的判断，则是将测试样本点x拟合到T颗决策树。计算在每颗决策树上该样本的叶子节点的深度ht(x),从而可以计算出平均高度h(x)。此时我们用下面的公式计算样本点x的异常概率:

	s(x,m)=2−h(x)c(m)

	其中，m为样本个数。

	c(m)的表达式为：

	c(m)=2ln(m−1)+ξ−2m−1m,ξ为欧拉常数

	s(x,m)的取值范围是[0,1],取值越接近于1，则是异常点的概率也越大。

### （四）随机森林的优缺点

- 优点

  1） 训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。

  2） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。

  3） 在训练后，可以给出各个特征对于输出的重要性

  4） 由于采用了随机采样，训练出的模型的方差小，泛化能力强。

  5） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。

  6） 对部分特征缺失不敏感。

- 缺点

  1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。

  2)   取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。

### （五）随机森林在sklearn中的参数解释

- sklearn.ensemble.RandomForestClassifier

    class sklearn.ensemble.RandomForestClassifier(n_estimators=’warn’, criterion=’gini’, max_depth=None, 
    
    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, 
    
    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, 
    
    oob_score=False, n_jobs=None, random_state=None, verbose=0, 
    
    warm_start=False, class_weight=None)[source]

- `n_estimators`:森林中树的数量，初始越多越好，但是会增加训练时间，到达一定数量后模型的表现不会再有显著的提升

- `criterion`:特征选择的标准，有信息增益和基尼系数两种，使用信息增益的是ID3和C4.5算法（使用信息增益比），使用基尼系数的CART算法，默认是gini系数。


- `max_depth`:决策树最大深度，决策树模型先对所有数据集进行切分，再在子数据集上继续循环这个切分过程，max_depth可以理解成用来限制这个循环次数。

- `min_samples_split`:子数据集再切分需要的最小样本量，默认是2，如果子数据样本量小于2时，则不再进行下一步切分。如果数据量较小，使用默认值就可，如果数据量较大，为降低计算量，应该把这个值增大，即限制子数据集的切分次数。

- `min_samples_leaf=1`:叶节点（子数据集）最小样本数，如果子数据集中的样本数小于这个值，那么该叶节点和其兄弟节点都会被剪枝（去掉），该值默认为1。

- `min_weight_fraction_leaf`:在叶节点处的所有输入样本权重总和的最小加权分数，如果不输入则表示所有的叶节点的权重是一致的。

- `max_features`:特征切分时考虑的最大特征数量，默认是对所有特征进行切分，也可以传入int类型的值，表示具体的特征个数；也可以是浮点数，则表示特征个数的百分比；还可以是sqrt,表示总特征数的平方根；也可以是log2，表示总特征数的log个特征。


- `max_leaf_nodes`:最大叶节点个数，即数据集切分成子数据集的最大个数。

- `min_impurity_decrease`:切分点不纯度最小减少程度，如果某个结点的不纯度减少小于这个值，那么该切分点就会被移除。

- `min_impurity_split`:切分点最小不纯度，用来限制数据集的继续切分（决策树的生成），如果某个节点的不纯度（可以理解为分类错误率）小于这个阈值，那么该点的数据将不再进行切分。


- `bootstrap=True`:bootstrap采样，默认为True


- `oob_score=False`:oob（out of band，带外）数据，即：在某次决策树训练中没有被bootstrap选中的数据。多单个模型的参数训练，我们知道可以用cross validation（cv）来进行，但是特别消耗时间，而且对于随机森林这种情况也没有大的必要，所以就用这个数据对决策树模型进行验证，算是一个简单的交叉验证。性能消耗小，但是效果不错。


- `n_jobs=None`:n_jobs,并行化，可以在机器的多个核上并行的构造树以及计算预测值，不过受限于通信成本，可能效率并不会说分为k个线程就得到k倍的提升，不过整体而言相对需要构造大量的树或者构建一棵复杂的树而言还是高效的.


- `random_state=None`:随机数种子，类似于train_test_split分割时所使用的random_state


- `verbose=0`:是否显示任务进程。


- `class_weight`:权重设置，主要是用于处理不平衡样本，与LR模型中的参数一致，可以自定义类别权重，也可以直接使用balanced参数值进行不平衡样本处理。

#### python实现
- 导入所需库和生成分类数据集

    
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.datasets import make_classification

    X, y = make_classification(n_samples=1000, n_features=4,
              					n_informative=2, n_redundant=0,
       							random_state=0, shuffle=False)

- 初始化RandomForest，训练模型

 	`clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)`

    `clf.fit(X, y)`

    
    out:RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
       max_depth=2, max_features='auto', max_leaf_nodes=None,
       min_impurity_decrease=0.0, min_impurity_split=None,
       min_samples_leaf=1, min_samples_split=2,
       min_weight_fraction_leaf=0.0, n_estimators=100,
       n_jobs=None, oob_score=False, random_state=0, verbose=0,
       warm_start=False)
    

    print(clf.feature_importances_)
[0.14205973 0.76664038 0.0282433  0.06305659]

    print(clf.predict([[0, 0, 0, 0]]))
[1]
    
### （六）随机森林的应用场景



## Python实现

    from sklearn.datasets import load_boston
     
    boston = load_boston()
     
    from sklearn.model_selection import train_test_split
     
    import numpy as np
     
    X = boston.data
    y = boston.target
     
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 33, test_size = 0.25)
     
    print('The max target value is: ', np.max(boston.target))
    print('The min target value is: ', np.min(boston.target))
    print('The average terget value is: ', np.mean(boston.target))
     
    from sklearn.preprocessing import StandardScaler
     
    ss_X = StandardScaler()
    ss_y = StandardScaler()
     
    ss_X.fit(X_train)
    X_train = ss_X.transform(X_train)
    X_test = ss_X.transform(X_test)
    ss_y.fit(y_train.reshape(-1,1))
    y_train = ss_y.transform(y_train.reshape(-1,1))
    y_test = ss_y.transform(y_test.reshape(-1,1))
     
    from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor
     
    rfr = RandomForestRegressor()
    rfr.fit(X_test, y_test)
    rfr_y_predict = rfr.predict(X_test)
     
    etr = ExtraTreesRegressor()
    etr.fit(X_train, y_train)
    etr_y_predict = etr.predict(X_test)
     
    gbr = GradientBoostingRegressor()
    gbr.fit(X_train, y_train)
    gbr_y_predict = gbr.predict(X_test)
     
    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
     
    print('R-squared value of RandomForestRegressor is: ', rfr.score(X_test, y_test))
    print('The mean squared error of RandomForestRegressor is: ', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict)))
    print('The mean absolute error of RandomForestRegressor is: ', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(rfr_y_predict)))
     
    print('R-squared of ExtraTreesRegressor is: ', etr.score(X_test, y_test))
    print('the value of mean squared error of ExtraTreesRegressor is: ', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict)))
    print('the value of mean ssbsolute error of ExtraTreesRegressor is: ', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(etr_y_predict)))
     
    print('R-squared of GradientBoostingRegressor is: ', gbr.score(X_test, y_test))
    print('the value of mean squared error of GradientBoostingRegressor is: ', mean_squared_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict)))
    print('the value of mean ssbsolute error of GradientBoostingRegressor is: ', mean_absolute_error(ss_y.inverse_transform(y_test), ss_y.inverse_transform(gbr_y_predict)))


The max target value is:  50.0

The min target value is:  5.0

The average terget value is:  22.532806324110677



R-squared value of RandomForestRegressor is:  0.9260981327559928

The mean squared error of RandomForestRegressor is:  5.730446456692913

The mean absolute error of RandomForestRegressor is:  1.3441732283464565


R-squared of ExtraTreesRegressor is:  0.7795066758722334

the value of mean squared error of ExtraTreesRegressor is:  17.097337795275596

the value of mean ssbsolute error of ExtraTreesRegressor is:  2.5681102362204724


R-squared of GradientBoostingRegressor is:  0.8378994824138197

the value of mean squared error of GradientBoostingRegressor is:  12.569483982898165

the value of mean ssbsolute error of GradientBoostingRegressor is:  2.294410992825806


## 参考链接

1、周志华.机器学习

2、https://www.cnblogs.com/burton/p/10460935.html

3、https://www.cnblogs.com/pinard/p/6156009.html

​	





